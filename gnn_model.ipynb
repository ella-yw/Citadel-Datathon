{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This is the Graph Neural Network model!"],"metadata":{"id":"9Rc6uVdtpzkc"}},{"cell_type":"markdown","source":["## 1. Set device to GPU"],"metadata":{"id":"vykBVi1HqGGJ"}},{"cell_type":"markdown","source":["Click `Runtime` and then under `Change runtime type` set `hardware accelerator` to GPU."],"metadata":{"id":"v9eDk1ASqntP"}},{"cell_type":"markdown","source":["## 2. Import necessary packages"],"metadata":{"id":"wrMNfSXop8T5"}},{"cell_type":"markdown","source":["Import `torch` and `os`."],"metadata":{"id":"FcuuVrD6qr4F"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_QaXhE2xpuwQ","executionInfo":{"status":"ok","timestamp":1690599531514,"user_tz":240,"elapsed":5962,"user":{"displayName":"Andy Jiang","userId":"09434980078266214903"}},"outputId":"72a780ed-8063-436e-e6c8-b1fa51d51878"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch has version 2.0.1+cu118\n"]}],"source":["import torch\n","import os\n","print(\"PyTorch has version {}\".format(torch.__version__))"]},{"cell_type":"markdown","source":["Import `torch-scatter` and `torch-sparse`, which are packages necessary for `torch-geometric` (PyTorch Geometric, the library for graph machine learning). Also import `ogb`, the Open Graph Benchmark library, to access graph datasets."],"metadata":{"id":"K_Pwyq5yqEV5"}},{"cell_type":"code","source":["!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n","!pip install torch-geometric\n","!pip install ogb\n","\n","from IPython.display import clear_output\n","clear_output()"],"metadata":{"id":"VyD8Ff-wqiRP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Read in loaded and proprocessed dataset (TODO)"],"metadata":{"id":"2X0mlCzarBVN"}},{"cell_type":"code","source":["import pandas as pd\n","import torch.nn.functional as F\n","import torch_geometric.transforms as T\n","from ogb.linkproppred import PygLinkPropPredDataset, Evaluator"],"metadata":{"id":"L5HBO9A6sCwR","executionInfo":{"status":"ok","timestamp":1690497697780,"user_tz":240,"elapsed":2010,"user":{"displayName":"Andy Jiang","userId":"09434980078266214903"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab443d64-5923-496d-a112-de5eb7edfbfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at4_ops6narrow4callERKNS_6TensorElll\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"]}]},{"cell_type":"markdown","source":["The `make_symmetric_sparse` method takes a sparse tensor and makes it symmetric."],"metadata":{"id":"UVw905ax__wl"}},{"cell_type":"code","source":["import torch\n","\n","def make_symmetric_sparse(tensor):\n","    # Extract the row, column, and value data from the sparse tensor\n","    row = tensor.crow_indices()\n","    col = tensor.col_indices()\n","    value = tensor.values()\n","\n","    # Create a symmetric index array by stacking row and column indices in reverse order\n","    sym_row = torch.cat([row, col], dim=0)\n","    sym_col = torch.cat([col, row], dim=0)\n","\n","    # Combine the value tensor with itself to get the symmetric values\n","    sym_value = torch.cat([value, value], dim=0)\n","\n","    # Check for duplicates in the symmetric index array\n","    sym_indices = torch.stack([sym_row, sym_col], dim=0)\n","    _, unique_idx = torch.unique(sym_indices, dim=1, return_inverse=True)\n","    unique_idx = unique_idx.view(-1)\n","\n","    # Remove duplicates from the symmetric index array\n","    sym_row = sym_row[unique_idx]\n","    sym_col = sym_col[unique_idx]\n","    sym_value = sym_value[unique_idx]\n","\n","    # Create the symmetric sparse tensor\n","    symmetric_tensor = torch.sparse_coo_tensor(\n","        indices=torch.stack([sym_row, sym_col], dim=0),\n","        values=sym_value,\n","        size=tensor.size(),\n","    )\n","\n","    return symmetric_tensor\n"],"metadata":{"id":"Fl2VB5Q5zRyJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For now, we're using a graph and datasets provided by `ogb`. Eventualy, we'll create our own graph and datasets using the airline datasets."],"metadata":{"id":"1bGNbdn-AMxb"}},{"cell_type":"code","source":["dataset_name = 'ogbl-arxiv'\n","dataset = PygLinkPropPredDataset(name=dataset_name,\n","                                  transform=T.ToSparseTensor())\n","\n","data = dataset[0]\n","\n","a = data.adj_t\n","\n","print(a)\n","print(type(a))\n","print(a.crow_indices())\n","print(a.col_indices())\n","print(a.values())\n","print(a.crow_indices().shape)\n","print(a.col_indices().shape)\n","print(a.values().shape)\n","\n","# symmetric_tensor = make_symmetric_sparse(a)\n","# data.adj_t = symmetric_tensor\n","# data.adj_t = data.adj_t.coalesce()\n","\n","'''\n","n = a.shape[0]\n","for i in range(n):\n","  for j in range(n):\n","    if a[i, j] != a[j, i]:\n","      print(i, j, a[i, j], a[j, i])\n","'''\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# If you use GPU, the device should be cuda\n","print('Device: {}'.format(device))\n","\n","data = data.to(device)\n","split_idx = dataset.get_idx_split()\n","train_idx = split_idx['train'].to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CEoEuM0hrzS3","executionInfo":{"status":"ok","timestamp":1690498954824,"user_tz":240,"elapsed":1533,"user":{"displayName":"Andy Jiang","userId":"09434980078266214903"}},"outputId":"c46c3862-e4c0-4d25-fd94-4ead5ef72cb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(crow_indices=tensor([      0,     289,     290,  ..., 1166240,\n","                            1166243, 1166243]),\n","       col_indices=tensor([   411,    640,   1162,  ...,  30351,  35711,\n","                           103121]),\n","       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]), size=(169343, 169343),\n","       nnz=1166243, layout=torch.sparse_csr)\n","<class 'torch.Tensor'>\n","tensor([      0,     289,     290,  ..., 1166240, 1166243, 1166243])\n","tensor([   411,    640,   1162,  ...,  30351,  35711, 103121])\n","tensor([1., 1., 1.,  ..., 1., 1., 1.])\n","torch.Size([169344])\n","torch.Size([1166243])\n","torch.Size([1166243])\n","Device: cuda\n"]}]},{"cell_type":"markdown","source":["## 4. Create Models"],"metadata":{"id":"MRnJWCL-rG8t"}},{"cell_type":"markdown","source":["For now, we can stick with GCN (Graph Convolutional Network) and GAT (Graph Attention Network). May implement GIN in the future, but it's more complicated."],"metadata":{"id":"oeyzOzcKt1bi"}},{"cell_type":"code","source":["from torch_geometric.nn import GCNConv, GINConv, GATConv"],"metadata":{"id":"CUpy_d5st6ok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GCN"],"metadata":{"id":"sOnUJ3ljBQIB"}},{"cell_type":"code","source":["class GCN(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n","                 dropout, return_embeds=False):\n","        super(GCN, self).__init__()\n","\n","        # A list of GCNConv layers\n","        self.convs = torch.nn.ModuleList(\n","            [GCNConv(in_channels=input_dim, out_channels=hidden_dim)] +\n","            [GCNConv(in_channels=hidden_dim, out_channels=hidden_dim)\n","                for i in range(num_layers-2)] +\n","            [GCNConv(in_channels=hidden_dim, out_channels=output_dim)]\n","        )\n","\n","        # A list of 1D batch normalization layers\n","        self.bns = torch.nn.ModuleList([\n","            torch.nn.BatchNorm1d(num_features=hidden_dim)\n","                for i in range(num_layers-1)\n","        ])\n","\n","\n","        # The log softmax layer\n","        self.softmax = torch.nn.LogSoftmax()\n","\n","        # Probability of an element to be zeroed\n","        self.dropout = dropout\n","\n","        # Skip classification layer and return node embeddings\n","        self.return_embeds = return_embeds\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def forward(self, x, adj_t):\n","        for conv, bn in zip(self.convs[:-1], self.bns):\n","            x1 = F.relu(bn(conv(x, adj_t)))\n","            if self.training:\n","                x1 = F.dropout(x1, p=self.dropout)\n","            x = x1\n","        x = self.convs[-1](x, adj_t)\n","        out = x if self.return_embeds else self.softmax(x)\n","\n","        return out"],"metadata":{"id":"7ZhUT-FMtp0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GIN"],"metadata":{"id":"hEpvOwvhBSl9"}},{"cell_type":"code","source":["class GIN(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n","                 dropout, return_embeds=False):\n","        super(GIN, self).__init__()\n","\n","        # A list of GINConv layers\n","        self.convs = torch.nn.ModuleList(\n","            [GINConv(in_channels=input_dim, out_channels=hidden_dim)] +\n","            [GINConv(in_channels=hidden_dim, out_channels=hidden_dim)\n","                for i in range(num_layers-2)] +\n","            [GINConv(in_channels=hidden_dim, out_channels=output_dim)]\n","        )\n","\n","        # A list of 1D batch normalization layers\n","        self.bns = torch.nn.ModuleList([\n","            torch.nn.BatchNorm1d(num_features=hidden_dim)\n","                for i in range(num_layers-1)\n","        ])\n","\n","\n","        # The log softmax layer\n","        self.softmax = torch.nn.LogSoftmax()\n","\n","        # Probability of an element to be zeroed\n","        self.dropout = dropout\n","\n","        # Skip classification layer and return node embeddings\n","        self.return_embeds = return_embeds\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def forward(self, x, adj_t):\n","        for conv, bn in zip(self.convs[:-1], self.bns):\n","            x1 = F.relu(bn(conv(x, adj_t)))\n","            if self.training:\n","                x1 = F.dropout(x1, p=self.dropout)\n","            x = x1\n","        x = self.convs[-1](x, adj_t)\n","        out = x if self.return_embeds else self.softmax(x)\n","\n","        return out"],"metadata":{"id":"9mD78yas6qiR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GAT"],"metadata":{"id":"doBGYEGCBUXd"}},{"cell_type":"code","source":["class GAT(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n","                 dropout, return_embeds=False):\n","        super(GAT, self).__init__()\n","\n","        # A list of GINConv layers\n","        self.convs = torch.nn.ModuleList(\n","            [GATConv(in_channels=input_dim, out_channels=hidden_dim)] +\n","            [GATConv(in_channels=hidden_dim, out_channels=hidden_dim)\n","                for i in range(num_layers-2)] +\n","            [GATConv(in_channels=hidden_dim, out_channels=output_dim)]\n","        )\n","\n","        # A list of 1D batch normalization layers\n","        self.bns = torch.nn.ModuleList([\n","            torch.nn.BatchNorm1d(num_features=hidden_dim)\n","                for i in range(num_layers-1)\n","        ])\n","\n","\n","        # The log softmax layer\n","        self.softmax = torch.nn.LogSoftmax()\n","\n","        # Probability of an element to be zeroed\n","        self.dropout = dropout\n","\n","        # Skip classification layer and return node embeddings\n","        self.return_embeds = return_embeds\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def forward(self, x, adj_t):\n","        for conv, bn in zip(self.convs[:-1], self.bns):\n","            x1 = F.relu(bn(conv(x, adj_t)))\n","            if self.training:\n","                x1 = F.dropout(x1, p=self.dropout)\n","            x = x1\n","        x = self.convs[-1](x, adj_t)\n","        out = x if self.return_embeds else self.softmax(x)\n","\n","        return out"],"metadata":{"id":"rGN51_uA7Tqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### GAT and GCN Hybrid"],"metadata":{"id":"KxfhTF3YmODP"}},{"cell_type":"code","source":["class GATC(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n","                 dropout, return_embeds=False):\n","        super(GATC, self).__init__()\n","\n","        # A list of GINConv layers\n","        self.convs = torch.nn.ModuleList(\n","            [GATConv(in_channels=input_dim, out_channels=hidden_dim)] +\n","            [GATConv(in_channels=hidden_dim, out_channels=hidden_dim)\n","                for i in range(num_layers-3)] +\n","            [GCNConv(in_channels=hidden_dim, out_channels=hidden_dim)]\n","        )\n","\n","        # A list of 1D batch normalization layers\n","        self.bns = torch.nn.ModuleList([\n","            torch.nn.BatchNorm1d(num_features=hidden_dim)\n","                for i in range(num_layers-1)\n","        ])\n","\n","        self.lstm = torch.nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim)\n","\n","\n","        # The log softmax layer\n","        self.softmax = torch.nn.LogSoftmax()\n","\n","        # Probability of an element to be zeroed\n","        self.dropout = dropout\n","\n","        # Skip classification layer and return node embeddings\n","        self.return_embeds = return_embeds\n","\n","    def reset_parameters(self):\n","        for conv in self.convs:\n","            conv.reset_parameters()\n","        for bn in self.bns:\n","            bn.reset_parameters()\n","\n","    def forward(self, x, adj_t):\n","        x = self.lstm(x)\n","        for conv, bn in zip(self.convs[:-1], self.bns):\n","            x1 = F.relu(bn(conv(x, adj_t)))\n","            if self.training:\n","                x1 = F.dropout(x1, p=self.dropout)\n","            x = x1\n","        x = self.convs[-1](x, adj_t)\n","\n","        out = x if self.return_embeds else self.softmax(x)\n","\n","        return out"],"metadata":{"id":"_AOMdm77HQ6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Define the `train` and `test` functions"],"metadata":{"id":"AL_Y7P1bBXDw"}},{"cell_type":"code","source":["def train(model, data, train_idx, optimizer, loss_fn):\n","    model.train()\n","    loss = 0\n","\n","    optimizer.zero_grad()\n","    out = model(data.x, data.adj_t)\n","    loss = loss_fn(out[train_idx], data.y[train_idx].reshape(-1))\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()"],"metadata":{"id":"qAtrmo88uL8u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@torch.no_grad()\n","def test(model, data, split_idx, evaluator, save_model_results=False):\n","    model.eval()\n","\n","    # The output of model on all data\n","    out = None\n","\n","    out = model(data.x, data.adj_t)\n","\n","    y_pred = out.argmax(dim=-1, keepdim=True)\n","\n","    train_acc = evaluator.eval({\n","        'y_true': data.y[split_idx['train']],\n","        'y_pred': y_pred[split_idx['train']],\n","    })['acc']\n","    valid_acc = evaluator.eval({\n","        'y_true': data.y[split_idx['valid']],\n","        'y_pred': y_pred[split_idx['valid']],\n","    })['acc']\n","    test_acc = evaluator.eval({\n","        'y_true': data.y[split_idx['test']],\n","        'y_pred': y_pred[split_idx['test']],\n","    })['acc']\n","\n","    if save_model_results:\n","      print (\"Saving Model Predictions\")\n","\n","      data = {}\n","      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n","\n","      df = pd.DataFrame(data=data)\n","      # Save locally as csv\n","      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n","\n","\n","    return train_acc, valid_acc, test_acc"],"metadata":{"id":"d9sLgkC6uPWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Set arguments for model"],"metadata":{"id":"lFegFrShuay8"}},{"cell_type":"code","source":["args = {\n","    'device': device,\n","    'num_layers': 3,\n","    'hidden_dim': 128,\n","    'dropout': 0.5,\n","    'lr': 0.01,\n","    'epochs': 100,\n","}"],"metadata":{"id":"LimxJzrBuaQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Initialize model and evaluator"],"metadata":{"id":"Qt1vrYEdupzJ"}},{"cell_type":"markdown","source":["Choose the model type from among `GCN`, `GAT`, `GIN`, and `GATC`:"],"metadata":{"id":"-zKnjkvFI0ZC"}},{"cell_type":"code","source":["model_type = GAT\n","\n","model = model_type(data.num_features, args['hidden_dim'],\n","            dataset.num_classes, args['num_layers'],\n","            args['dropout']).to(device)\n","evaluator = Evaluator(name=dataset_name)"],"metadata":{"id":"nQAs_unBupcM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Train the model"],"metadata":{"id":"kfS7fOZ1vAFn"}},{"cell_type":"code","source":["import copy\n","\n","# reset the parameters to initial random value\n","model.reset_parameters()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n","loss_fn = F.nll_loss\n","\n","best_model = None\n","best_valid_acc = 0\n","\n","for epoch in range(1, 1 + args[\"epochs\"]):\n","  loss = train(model, data, train_idx, optimizer, loss_fn)\n","  result = test(model, data, split_idx, evaluator)\n","  train_acc, valid_acc, test_acc = result\n","  if valid_acc > best_valid_acc:\n","      best_valid_acc = valid_acc\n","      best_model = copy.deepcopy(model)\n","  print(f'Epoch: {epoch:02d}, '\n","        f'Loss: {loss:.4f}, '\n","        f'Train: {100 * train_acc:.2f}%, '\n","        f'Valid: {100 * valid_acc:.2f}% '\n","        f'Test: {100 * test_acc:.2f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"oDS6UNY2vB5A","executionInfo":{"status":"error","timestamp":1690498746742,"user_tz":240,"elapsed":274,"user":{"displayName":"Andy Jiang","userId":"09434980078266214903"}},"outputId":"7f65aa3d-6163-4a2d-b3db-e6a8781a9ce1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-fb35fbceec90>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-9a199fe621dd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, train_idx, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-141fc7846fb9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Static graphs not supported in 'GATConv'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mx_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_src\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Tuple of source and target node features:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mx_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 15.77 GiB total capacity; 11.93 GiB already allocated; 654.12 MiB free; 14.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]},{"cell_type":"markdown","source":["## 9. Get the best model"],"metadata":{"id":"NbBRSU7ovcMv"}},{"cell_type":"code","source":["best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n","train_acc, valid_acc, test_acc = best_result\n","print(f'Best model: '\n","      f'Train: {100 * train_acc:.2f}%, '\n","      f'Valid: {100 * valid_acc:.2f}% '\n","      f'Test: {100 * test_acc:.2f}%')"],"metadata":{"id":"Urk8-1rpvb0C"},"execution_count":null,"outputs":[]}]}